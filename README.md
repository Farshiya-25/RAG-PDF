# RAG with PDF

This project implements a **Retrieval-Augmented Generation (RAG)** system that allows users to upload PDF files, index their content using Pinecone, and query the content through a **Streamlit** web interface.  
The answers are generated by Google's **Gemini LLM**, with context retrieved from Pinecone's vector search.

## Features

- üì§ **PDF Upload**: Upload any PDF directly via the Streamlit interface.
- ‚úÇ **Text Chunking**: Automatically splits large PDFs into smaller, manageable chunks for embedding.
- üîç **Vector Indexing**: Creates embeddings for each chunk and stores them in Pinecone.
- üì¶ **Index Management**:
  - Creates a new index if it doesn‚Äôt already exist.
  - Inserts vectors only if they are not already stored.
- üß† **RAG-based QA**:
  - Embeds the user query using the same embedding model.
  - Retrieves top matching chunks from Pinecone.
  - Passes retrieved context + query to Gemini LLM using a customized propmt.
- üí¨ **Interactive Q&A**: Streamlit UI to type questions and view answers instantly.

## Tech Stack

- **Python** ‚Äì Core programming language
- **Streamlit** ‚Äì Web UI for file uploads and Q&A
- **Pinecone** ‚Äì Vector database for semantic search
- **Google Gemini API** ‚Äì Large Language Model for answer generation
- **Text Splitter** ‚Äì For chunking and managing text embeddings
- **PyPDF** ‚Äì Extracts text from PDF files


## Workflow

1. **Upload PDF**
   - Extracts text using `PyPDF2` or `pypdf`.
   - Splits text into overlapping chunks.

2. **Vectorization & Storage**
   - Generates embeddings using Gemini embedding model.
   - Checks Pinecone index:
     - Creates index if missing.
     - Inserts vectors only if absent.

3. **Query Handling**
   - User types a question in Streamlit.
   - Generates query embedding.
   - Retrieves top matching chunks from Pinecone.
   - Builds a prompt with retrieved context + question.
   - Sends prompt to Gemini for final answer generation.

4. **Answer Display**
   - Shows Gemini's generated answer in Streamlit UI.
   
## UI Preview

- **Step 1:** Upload PDF
- **Step 2:** Ask your question in the input box
- **Step 3:** Get an AI-generated answer instantly
---

## Environment Variables

To run this project, you will need to add the following environment variables to your .env file

PINECONE_API_KEY=your_pinecone_api_key

GOOGLE_API_KEY=your_gemini_api_key

---

## requirements.txt

```txt
streamlit
pinecone
pypdf
python-dotenv
google-generativeai
langchain

```
---

## Installation

```bash
# Clone the repository
git clone https://gitlab.siamcomputing.com/siamcomputing-training/cet/machine-learning/ml-interns/2025/kathija/rag-pdf.git

cd pdf-rag-pinecone-gemini
# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate   # For Linux/Mac
venv\Scripts\activate      # For Windows

# Install dependencies
pip install -r requirements.txt
```
## Run the app
streamlit run rag_chatbot.py

## Demo

[![Watch the Demo](https://img.youtube.com/vi/VIDEO_ID/0.jpg)](https://drive.google.com/file/d/1Tlo14ypRI5neFuraq-3DQ1FHfYp2ij1F/view?usp=sharing)
